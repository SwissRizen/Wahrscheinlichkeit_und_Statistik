\section{Joint Random Variables}
\begin{mainbox}{}
    Die \textbf{Joint Cum. Distribution Function} von $n$ Random Variables $X_1, \ldots, X_n$ (continuous oder discrete) ist die Abbildung $F: \R^n \to [0,1]$,
    $$(x_1, \ldots, x_n) \mapsto F(x_1, \ldots, x_n) := \P(X_1 \leq x_1, \ldots, X_n \leq x_n)$$
\end{mainbox}
\subsection{Discreter Fall - weight function}
    Für $n$ discrete RV $X_1, \ldots, X_n$ definieren wir ihre \textbf{Joint weight function} $p: \R^n \to [0,1]$ durch
    $$p(x_1, \ldots, x_n) := \P(X_1 = x_1, \ldots, X_n = x_n)$$
    Aus der Jointn weight function $p$ bekommt man die Joint Cum. Distribution Function mit
    \begin{align*}
        F(x_1, \ldots, x_n) &= \P(X_1 \leq x_1, \ldots, X_n \leq x_n)\\
        &= \sum_{y_1 \leq x_1, \ldots, y_n \leq x_n}\P(X_1 = y_1, \ldots, X_n = y_n)\\
        &= \sum_{y_1 \leq x_1, \ldots, y_n \leq x_n}p(y_1, \ldots,y_n)
    \end{align*}    

\begin{subbox}{}
    Seien $X_1, \ldots, X_n$ \tbf{discrete} Random Variables in $(\Omega, \F, \P)$, sodass $X_1 \in W_1, \ldots, X_n \in W_n$ a.s. für $W_1, \ldots, W_n \subset \R$ finite oder countable.

    Für $\phi: \R^n \to \R$ beliebig, ist $Z = \phi(X_1, \ldots, X_n)$ eine discrete Random Variable mit $Z \in W = \phi(W_1 \times \ldots \times W_n)$ a.s. . 

    Die weight function von $Z$ ist gegeben durch $p_Z: W \to [0,1]$:
    $$p_Z(t) := \P(Z = t) = \sum_{\substack{x_1 \in W_1, \ldots, x_n \in W_n\\ \phi(x_1, \ldots, x_n) = t}} p(x_1, \ldots, x_n)$$
\end{subbox}
\begin{enumerate}
    \item Mit dem vorherigen Satz können wir aus der Jointn Distribution die \textbf{Marginal Distribution} einer Random Variables extrahieren (wegsummieren). Wir verwenden dafür einfach die Funktion 
    $$\phi(x_1, \ldots, x_n) = x_i$$
    \item Der Expected Value des Bildes der Funktion $\phi: \R^n \to \R$ ist
    $$\E(\phi(X_1, \ldots, X_n)) = \sum_{x_1, \ldots, x_n}\phi(x_1, \ldots, x_n)p(x_1, \ldots, x_n)$$
    \item Wir haben eine Äquvalenz: 
    \begin{align*}
        &X_1, \ldots, X_n \text{independent}\\
        &\quad \iff \\
        &\forall x_1 \in W_1, \ldots, x_n \in W_n\\
        &p(x_1, \ldots, x_n) = \P(X_1 = x_1) \cdot \ldots \cdot \P(X_n = x_n)
    \end{align*}
\end{enumerate}

\subsection{Continuousr Fall - Gemeinsame Density}
\begin{mainbox}{Gemeinsame Density}
    Falls die Joint Cum. Distribution Function von $n$ Random Variables $X_1, \ldots, X_n$ sich schreiben lässt als
    $$F(x_1, \ldots, x_n) = \int_{-\infty}^{x_1} \cdots \int_{-\infty}^{x_n}f(t_1, \ldots, t_n) \mathop{dt_n}\ldots\mathop{dt_1}$$
    für eine Funktion $f: \R^n \to [0, \infty)$, so heisst $f(x_1, \ldots, x_n)$ die \textbf{Joint Density} von $X_1, \ldots X_n$.
\end{mainbox}
   
\begin{enumerate}
    \item $f(x_1, \ldots, x_n) \geq 0$, und $ = 0$ ausserhalb von $\mathcal{W}(X_1, \ldots, X_n)$.
    \item 
    \begin{gather*}
        \P((X_1, \ldots, X_n) \in A) = \idotsint\limits_{(x_1, \ldots, x_n) \in A} f(x_1, \ldots, x_n) \mathop{dx_n}\ldots\mathop{dx_1}
    \end{gather*}
    für $A \subseteq \R^n$ beliebig.
    \item Haben $X, Y$ die Joint Cum. Distribution Function $F_{X,Y}$, so ist $F_X: \R \to [0,1]$,
    $$F_X(x) := \P(X \leq x) = \P(X \leq x, Y \leq \infty) = \lim_{y \to \infty}F_{X,Y}(x,y)$$
    die Cum. Distribution Function der \textit{Marginal Distribution} von $X$. Analoges gilt für $F_Y$.
    \item Falls $X,Y$ eine Joint Density $f(x,y)$ haben, so haben auch die Marginal Distributionen von $X$ und $Y$ Densityn $f_X: \R \to [0, \infty)$ und $F_Y: \R \to [0, \infty)$.
    $$f_X(x) = \int_{-\infty}^{\infty}f(x,y)\mathop{dy} \text{ bzw. } f_Y(y)=\int_{-\infty}^{\infty}f(x,y)\dx$$
    Die \textbf{Densityfunktion} einer Marginal Distribution (Marginal Density) entsteht aus der Jointn Densityfunktion durch ''Wegintegrieren'' der anderen Variable(n).
\end{enumerate}
Wenn $X_1, \ldots, X_n$ continuouse RV mit Densityn $f_1, \ldots, f_n$, dann sind die folgenden Aussagen äquivalent:
\begin{itemize}
    \item $X_1, \ldots, X_n$ independent
    \item $(X_1, \ldots, X_n)$ ist continuous mit Jointr Density $$f(x_1, \ldots, x_n) = f_1(x_1) \cdot \ldots \cdot f_n(x_n)$$
    \item Für alle $\phi_1: \R \to \R, \ldots, \phi_n: \R \to \R$ die stückweise continuous und bounded sind, gilt 
    $$\E(\phi_1(X_1)\cdot \ldots \cdot \phi_n(X_n)) = \E(\phi_1(X_1)) \cdot \ldots \cdot \E(\phi_n(X_n))$$
\end{itemize}
\subsection{Transformation von Random Variables}
\begin{subbox}{linearer Transformationssatz}
    Sei $Z$ ein $n$-dimensionaler Zufallsvektor und $g: (\R^n, \mathcal{B}^n) \to (\R^m, \mathcal{B}^m)$ eine messbare Abbildung. Dann ist
    $$H(\omega) = g(Z(\omega))$$
    ein $m$-dimensionaler Random Variable und ferner gilt
    $$\P(H \in A) = \P(Z \in g^{-1}(A)).$$
    Wenn $g$ linear und umkehrbar (i.e. $g(x) = m + Bx$ mit det$(B) \neq 0$) und unter Vorraussetzung, dass die Distribution von $Z$ absolut continuous ist, dann ist $H$ auch absolut continuous und es gilt:
    $$f_H(x)=\frac{1}{|\text{det}(B)|}f_Z(B^{-1}(x-m)).$$
\end{subbox}
\subsubsection*{Beispielrechnung}

$Z = (X, Y)$ $2$-dim Zufallsvektor. Wir wollen die Density von $X + Y$ berechnen.

Man wäre versucht die Matrix $B$ und den Vektor $m$ wie folgt zu definieren:
\begin{align*}
    &B= 
    \left(\begin{matrix}
        1 & 1
    \end{matrix}\right) 
    \text{ und } 
    m = \left(\begin{matrix}
        0\\
        0
    \end{matrix}\right)\\
    \implies &g((X,Y)) = 
    \left(\begin{matrix}
        1 & 1
    \end{matrix}\right) \cdot 
    \left(\begin{matrix}
        X\\
        Y
    \end{matrix}\right) = X + Y
\end{align*}
Dann wäre aber $B$ (und somit $g$) nicht invertierbar!
Deshalb wollen wir $B$ so wählen, dass $g((X,Y)) = (X, X + Y)$:
\begin{align*}
    &\left(\begin{matrix}
        X\\
        X + Y
    \end{matrix}\right) 
    = \left(\begin{matrix}
        1 & 0\\
        1 & 1
    \end{matrix}\right)
    \left(\begin{matrix}
        X\\
        Y
    \end{matrix}\right)\\
    \text{det}(B)&= 1 \neq 0 \implies B \text{ invertierbar}\\
    &B^{-1} = \left(\begin{matrix}
        1 & 0\\
        -1 & 1
    \end{matrix}\right)
\end{align*}
Nach dem linearen Transformationssatz gilt
\begin{align*}
    f_{X, X+Y}(x, z) &= \frac{1}{|\text{det}(B)|}f_{X, Y}\left(B^{-1}\cdot 
    \left(\begin{matrix}
        x\\
        z
    \end{matrix}\right)\right)\\
    &= 1 \cdot f_{X, Y}\left(\left(\begin{matrix}
        1 & 0\\
        -1 & 1
    \end{matrix}\right)\left(\begin{matrix}
        x\\
        z
    \end{matrix}\right)\right)\\
    &= f_{X, Y}\left(x, z-x\right)
\end{align*}
Aus der Jointn Density $f_{X, X + Y}$ können wir die Density $f_{X + Y}$ bestimmen.
\begin{align*}
    f_{X+Y}(z) &= \int_{-\infty}^{\infty}f_{X, X + Y}(x, z) \dx\\
    &= \int_{-\infty}^{\infty} f_{X,Y}(x, z-x) \dx\\
    \text{Falls $X$ und $Y$ independent}\\
    &= \int_{-\infty}^{\infty}f_X(x)\cdot f_Y(z-x) \dx
\end{align*}
\subsubsection{Charakterisierung der Density durch \(\mathbb{E}\)}
    Sei $\phi: \R^n \to \R$ eine Abbildung und $X_1, \ldots, X_n$ RV mit Jointr Density $f$. 
    Dann lässt sich $\E(Z)$ für die Random Variable $Z=\phi(X_1, \ldots, X_n)$ mit 
    $$\E(Z) =\idotsint\limits_{\R^n}\phi(x_1, \ldots, x_n)\cdot f(x_1, \ldots, x_n)\mathop{dx_n}\ldots\mathop{dx_1}$$
    berechnen.

Dies reicht aber nicht, um die Density einer transformierten RV zu berechnen. Joint Random Variables mit unterschiedlichen Densityn können den gleichen Expected Value haben.

\begin{subbox}{}
    Sei $f: \R \to \R_+$ eine Abbildung, sodass $\int_{-\infty}^{\infty}f(z)\mathop{dz} = 1$. Dann sind folgende Aussagen äquivalent
    \begin{itemize}
        \item $Z$ ist continuous mit Density $f$
        \item Für jede stückweise continuouse, boundede Abbildung $\psi: \R \to \R$ gilt 
        \[\E(\psi(Z)) = \int_{-\infty}^{\infty}\psi(z)f(z)\mathop{dz}\]
    \end{itemize}
\end{subbox}


\subsubsection*{Beispielrechnung}

Wir können diese Erkenntnis nutzen, um die Density einer transformierten Random Variable zu berechnen.

Seien $X$ und $Y$ zwei Random Variables mit Jointr Densityfunktion
$$f(x,y) = \begin{cases}
    \frac{1}{x^2y^2} & \text{ für } x \geq 1, y \geq 1\\
    0 & \text{ sonst.}
\end{cases}$$
Bestimme die Densityfunktion $f_V$ der Random Variable $V = XY$.

Sei $\psi: \R \to \R$ stückweise continuous und bounded.
Wir definieren $\phi(x, y) = \psi(xy) = \psi(v)$ und berechnen 
\begin{align*}
    \E(\psi(V)) = \E(\phi(X,Y)) &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\phi(x,y)f(x,y)\dx\mathop{dy}\\
    &= \int_{1}^{\infty}\int_{1}^{\infty}\psi(xy)\frac{1}{x^2y^2}\dx\mathop{dy}\\
    &\text{Substition } v = xy, \mathop{dv} = y\mathop{dx}\\
    &= \int_{1}^{\infty}\int_{y}^{\infty}\psi(v)\frac{1}{v^2}\frac{\mathop{dv}}{y}\mathop{dy}\\
    A = \{(v, y) \in \R^2 &\mid 1 \leq y < \infty, y \leq v < \infty\}\\
    = \{(v, y) \in \R^2 &\mid 1 \leq y \leq v, 1 \leq v < \infty\}\\
    \text{Zeichnung hilft ;)}\\
    &= \int_{1}^{\infty}\int_{1}^{v}\psi(v)\frac{1}{v^2y}\mathop{dy}\mathop{dv}\\
    &= \int_{1}^{\infty}\psi(v)\frac{\ln(v)}{v^2}\mathop{dv}\\
    &= \int_{-\infty}^{\infty}\psi(v) \cdot \frac{\ln(v)}{v^2}\mathds{1}_{v \in [1,\infty)}\mathop{dv}
\end{align*}
$$\implies f_V(t) = \frac{\ln(v)}{v^2}\mathds{1}_{v \in [1,\infty)}$$

\begin{mainbox}{genereller Transformationssatz}
    Sei $Z$ ein $n$-dimensionaler Zufallsvektor mit Densityfunktion $f_Z: \R^n \to \R_+$ und $\phi: \R^n \to \R^n$ continuous differenzierbar mit continuous differenzierbarer Umkehrabbildung $\phi^{-1}$. Dann gilt für die Density $f_U$ von $U = \phi(Z)$:
    \[f_U(\vec{u}) = f_Z\left(\phi^{-1}\left(\vec{u}\right)\right) \cdot |\text{det}(J_{\phi^{-1}}(\vec{u}))|\]
\end{mainbox}
\textbf{Beweisidee.} Für $A \subset \R^n$ gilt
\[\int_{A}f_U(\vec{u})\mathop{d\vec{u}} = \P(U \in A) = \P(Z \in \phi^{-1}(A)) = \int_{\phi^{-1}(A)}f_Z(\vec{z})\mathop{d\vec{z}}\]
Aus der mehrdimensionalen Integralrechnung folgt dann
\[\int_{\phi^{-1}(A)}f_Z(\vec{z})\mathop{d\vec{z}} = \int_{A}f_Z\left(\phi^{-1}\left(\vec{u}\right)\right) \cdot |\text{det}(J_{\phi^{-1}}(\vec{u}))|\mathop{d\vec{u}}\]
$\hfill\blacksquare$
\subsubsection*{Beispielrechnung}
Wir haben $Z = (X, Y)$, wobei $X, Y$ independent und exponentialverteilt mit $\lambda > 0$. Berechne die Densityfunktion $f_U$ von $$U:= \frac{X}{X+Y}$$
Wir definieren $\phi$, so dass $(U, Y) = \phi(X, Y)$.
\[\phi(x, y) = \left(\begin{matrix}
    \frac{x}{x+y}\\
    y
\end{matrix}\right) \text{ und } \phi^{-1}(u, y) = \left(\begin{matrix}
    \frac{uy}{1-u}\\
    y
\end{matrix}\right)\]
Check: $\phi^{-1}\left(\frac{x}{x+y}, y\right) = \left(\frac{\frac{x}{x+y}y}{1-\frac{x}{x+y}}, y\right) = \left(\frac{xy}{x+y-x}, y\right) = \left(x, y\right).$ 

We then have 
\begin{align*}
    \left|\text{det}\left(J_{\phi^{-1}}(u,y)\right)\right| &= \left\vert\text{det}\left(\begin{matrix}
        \frac{y}{1-u}+\frac{uy}{(1-u)^2} & 0\\
        \frac{u}{1-u} & 1
    \end{matrix}\right) \right\vert\\
    &= \left|\frac{y(1-u)+uy}{(1-u)^2}\right| = \left|\frac{y}{(1-u)^2}\right|
\end{align*}
Per genereller Transformationssatz gilt
\begin{align*}
    f_{U,Y}(u,y) &= f_{X, Y}\left(\frac{uy}{1-u}, y\right)\left\vert\frac{y}{(1-u)^2}\right\vert\\
                &= \begin{cases}
                    \lambda^2 e^{-\lambda\left(\frac{uy}{1-u}+y\right)}\left\vert\frac{y}{(1-u)^2}\right\vert &\text{if } \frac{uy}{1-u} \geq 0 \land y \geq 0\\
                    0 \cdot \left\vert\frac{y}{(1-u)^2}\right\vert & \text{sonst.}
                \end{cases}
\end{align*}
\begin{align*}
    f_U(u) &= \int_{-\infty}^\infty f_{U,Y}(u,y) \mathop{dy}\\
            &= \int_{0}^\infty \frac{\lambda^2}{(1-u)^2}e^{-\frac{\lambda}{1-u}y}y \mathds{1}_{u\in[0,1]}\mathop{dy}\\
            \text{per partielle Integration}\\
            &= \mathds{1}_{u \in [0,1]}
\end{align*}